{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "jV3B8tkdOA1f",
      "metadata": {
        "id": "jV3B8tkdOA1f"
      },
      "source": [
        "# **1. Importing the libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2c44e0fc",
      "metadata": {
        "id": "2c44e0fc"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nFWrF33zOOrw",
      "metadata": {
        "id": "nFWrF33zOOrw"
      },
      "source": [
        "# **2. Dataset loading and preprossesing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Tr3av15bO6sP",
      "metadata": {
        "id": "Tr3av15bO6sP"
      },
      "source": [
        "loading as utf8 encoding to change dataset into unicode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "873a72c9",
      "metadata": {
        "id": "873a72c9"
      },
      "outputs": [],
      "source": [
        "file  = open('1661-0.txt', 'r', encoding = \"utf8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UxphGx_CPVjn",
      "metadata": {
        "id": "UxphGx_CPVjn"
      },
      "source": [
        "append lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "18092b8a",
      "metadata": {
        "id": "18092b8a"
      },
      "outputs": [],
      "source": [
        "lines=[]\n",
        "for i in file:\n",
        "    lines.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHHw2TlcPngN",
      "metadata": {
        "id": "oHHw2TlcPngN"
      },
      "source": [
        "coverting data into one string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "440a3b97",
      "metadata": {
        "id": "440a3b97"
      },
      "outputs": [],
      "source": [
        "data=\"\"\n",
        "for i in lines:\n",
        "    data = ' '.join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sRdWdDV6P1Cz",
      "metadata": {
        "id": "sRdWdDV6P1Cz"
      },
      "source": [
        "cleaning up the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e92148c",
      "metadata": {
        "id": "2e92148c"
      },
      "outputs": [],
      "source": [
        " data = data.replace('\\n', '').replace('\\ufeff', '').replace('“','').replace('”','')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QNJE7ZvXP8fJ",
      "metadata": {
        "id": "QNJE7ZvXP8fJ"
      },
      "source": [
        "Splitting the entire dataset into each word in order without the presence of special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a01f9e91",
      "metadata": {
        "id": "a01f9e91"
      },
      "outputs": [],
      "source": [
        "data = data.split()\n",
        "data = ' '.join(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FtKFq-W-QSuK",
      "metadata": {
        "id": "FtKFq-W-QSuK"
      },
      "source": [
        "# **3.tokenizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a16f26f9",
      "metadata": {
        "id": "a16f26f9"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d29d39ff",
      "metadata": {
        "id": "d29d39ff"
      },
      "outputs": [],
      "source": [
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zITE7uYuQ0be",
      "metadata": {
        "id": "zITE7uYuQ0be"
      },
      "source": [
        "assiging perticular text a sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "32ef5b4f",
      "metadata": {
        "id": "32ef5b4f"
      },
      "outputs": [],
      "source": [
        "sequence_data = tokenizer.texts_to_sequences([data])[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mHVRIzEIRGfm",
      "metadata": {
        "id": "mHVRIzEIRGfm"
      },
      "source": [
        "# **4.sorting and vocab size **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "de0dad49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "de0dad49",
        "outputId": "6951cfd4-db4c-4d6c-ba22-5c82d0ab345b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "108958"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sequence_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "afb04b56",
      "metadata": {
        "id": "afb04b56"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9810f6ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9810f6ee",
        "outputId": "a4cc8b76-e758-4513-cede-299f393f79e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8624"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3GOCQu9TRV5V",
      "metadata": {
        "id": "3GOCQu9TRV5V"
      },
      "source": [
        "# **5.genrating next word**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7cc96627",
      "metadata": {
        "id": "7cc96627"
      },
      "outputs": [],
      "source": [
        "sequence=[]\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequence.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b2f04d30",
      "metadata": {
        "id": "b2f04d30"
      },
      "outputs": [],
      "source": [
        "sequence = np.array(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6cd04438",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6cd04438",
        "outputId": "ca51d4bc-2177-4fc3-b4eb-1d77a377f9a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 142, 4680,    1,  986],\n",
              "       [4680,    1,  986,    5],\n",
              "       [   1,  986,    5,  125],\n",
              "       ...,\n",
              "       [8623,    4,  347,   81],\n",
              "       [   4,  347,   81,  345],\n",
              "       [ 347,   81,  345, 1623]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "16cd722f",
      "metadata": {
        "id": "16cd722f"
      },
      "outputs": [],
      "source": [
        "X=[]\n",
        "y=[]\n",
        "\n",
        "for i in sequence:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ccda9f6b",
      "metadata": {
        "id": "ccda9f6b"
      },
      "outputs": [],
      "source": [
        "X=np.array(X)\n",
        "y=np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5eaf5871",
      "metadata": {
        "id": "5eaf5871"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2d904adc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2d904adc",
        "outputId": "30e736e9-fa6c-4add-fd46-97f2bfc746fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24748949",
      "metadata": {
        "id": "24748949"
      },
      "source": [
        "Building the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "962e9f31",
      "metadata": {
        "id": "962e9f31"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length = 3))\n",
        "model.add(LSTM(1000, return_sequences = True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "00d3ec6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "00d3ec6c",
        "outputId": "d08778a2-ba43-4c67-b86d-659d554b91b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 3, 10)             86240     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8624)              8632624   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,767,864\n",
            "Trainable params: 21,767,864\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe84900",
      "metadata": {
        "id": "8fe84900"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "qLdTOCsPRz51",
      "metadata": {
        "id": "qLdTOCsPRz51"
      },
      "source": [
        "training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2fa0c054",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2fa0c054",
        "outputId": "0b0a2306-08f0-4d0a-e89c-16468af0ed2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 27/413 [>.............................] - ETA: 3:22 - loss: 7.9207"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Prathamesh\\Downloads\\task A.ipynb Cell 36\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prathamesh/Downloads/task%20A.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m\"\u001b[39m\u001b[39mmodel weights.h5\u001b[39m\u001b[39m\"\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Prathamesh/Downloads/task%20A.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, optimizer \u001b[39m=\u001b[39m Adam(learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Prathamesh/Downloads/task%20A.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X, y, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m264\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint])\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\Prathamesh\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(\"model weights.h5\", monitor=\"loss\", verbose = 1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer = Adam(learning_rate = 0.001))\n",
        "model.fit(X, y, epochs=10, batch_size=264, callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e524c02a",
      "metadata": {
        "id": "e524c02a"
      },
      "outputs": [],
      "source": [
        "model = load_model('model weights.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def predict_word(model, tokenizer, text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = np.array(sequence)\n",
        "    preds=np.argmax(model.predict(sequence))\n",
        "    predicted_word = \"\"\n",
        "    for key, value in tokenizer.word_index.items():\n",
        "        if value == preds:\n",
        "            predicted_word = key\n",
        "            break\n",
        "    print(predicted_word)\n",
        "    return predicted_word\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LE3wT8D8R5rx",
      "metadata": {
        "id": "LE3wT8D8R5rx"
      },
      "source": [
        "examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "URpzsDfPLRkK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "URpzsDfPLRkK",
        "outputId": "71b3243b-dc7a-487d-877a-dc23fbd0bcfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 632ms/step\n",
            "a\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_word(model, tokenizer, \"I should have\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98-VtYvmLR7U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "98-VtYvmLR7U",
        "outputId": "0858beaa-63fd-4447-b915-7d61acaa6f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 717ms/step\n",
            "and\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_word(model, tokenizer, \"remained in our lodgings in Baker Street\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Lw67zuYLrzO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8Lw67zuYLrzO",
        "outputId": "97a08466-5754-459b-ce34-4ed0c3de50a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 27ms/step\n",
            "and\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_word(model, tokenizer, \"The Adventure of the Blue\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P5sXUhwaLrl9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "P5sXUhwaLrl9",
        "outputId": "dcc8fb72-7f01-4125-c59b-2e35340421f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 36ms/step\n",
            "and\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_word(model, tokenizer, \"lighting a cigarette\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OAml7m2pLSIE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OAml7m2pLSIE",
        "outputId": "cefb5772-88a1-46d9-d425-353a80dd35ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "of\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'of'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_word(model, tokenizer, \"The Adventure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VBGXX3rnR8Ev",
      "metadata": {
        "id": "VBGXX3rnR8Ev"
      },
      "source": [
        "endless conersion loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbZwMO8bKLS1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "bbZwMO8bKLS1",
        "outputId": "91b13e11-4dd8-40e9-ff26-12585dfbe3ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your line: I could not help laughing\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "that\n",
            "Enter your line: One night—it was\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "a\n",
            "Enter your line: I had seen little of Holmes\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "boone\n",
            "Enter your line: My own complete\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "statement\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-ca4bfce33a74>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your line: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"?\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    text = input(\"Enter your line: \")\n",
        "    if text==\"?\":\n",
        "        break\n",
        "    else:\n",
        "        text = text.split(\" \")\n",
        "        text = text[-3:]\n",
        "        predict_word(model, tokenizer, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e43ab0",
      "metadata": {
        "id": "59e43ab0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
